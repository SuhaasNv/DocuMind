# Database (required). Must match docker-compose: user, password, host, port, db name.
# Backend runs on HOST; use localhost so host can reach Docker-exposed ports.
# Start Postgres + pgvector with: docker compose up -d (from repo root).
DATABASE_URL="postgresql://user:password@localhost:5432/insight_garden"

# JWT (required for auth). MUST set to a long random string; app fails to start if default.
# Generate with: openssl rand -base64 32
JWT_SECRET="change-me-in-production-use-long-random-string"

# Optional: token expiry in seconds (default 604800 = 7 days)
# JWT_EXPIRES_IN=604800

# Redis (required for BullMQ). Backend runs on HOST; use localhost for Docker-exposed Redis.
REDIS_HOST=localhost
REDIS_PORT=6379
# REDIS_PASSWORD=

# Server
PORT=3000

# CORS: frontend origin for credentialed requests (e.g. Bearer token). Default in code is http://localhost:8080 if unset.
CORS_ORIGIN=http://localhost:8080

# Embeddings (for document chunk indexing)
# EMBEDDING_DIMENSION=1536
# EMBEDDING_PROVIDER=stub
# For OpenAI: EMBEDDING_PROVIDER=openai, OPENAI_API_KEY=sk-...
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small
# Note: migration uses vector(1536); if you change EMBEDDING_DIMENSION, add a new migration to alter the column.

# RAG / LLM (for document chat)
# Context construction: per-chunk truncation and total context cap
# MAX_CHUNK_CHARS=2000
# MAX_CONTEXT_CHARS=8000
# LLM_PROVIDER=stub
# Stub: no API key; returns placeholder. Use for tests or when no LLM is configured.
# Ollama: required for SSE streaming (POST /documents/:id/chat/stream). Non-streaming chat works with stub/openai too.
# OpenAI: non-streaming only; streaming throws 501. Use POST /documents/:id/chat for non-streaming.
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b
# OpenAI (non-streaming): OPENAI_API_KEY=sk-..., OPENAI_CHAT_MODEL=gpt-4o-mini
