# Database (required). Must match docker-compose for local dev; in production use your managed Postgres URL.
# Local: backend runs on HOST; use localhost so host can reach Docker-exposed ports.
# Start Postgres + pgvector locally: docker compose up -d (from repo root). Docker is local-only; production uses managed infra.
DATABASE_URL=""

# JWT (required for auth). MUST set to a long random string; app fails to start if default.
# Generate with: openssl rand -base64 32
JWT_SECRET="change-me-in-production-use-long-random-string"

# Optional: token expiry in seconds (default 604800 = 7 days)
# JWT_EXPIRES_IN=604800

# Redis (required for BullMQ). Backend runs on HOST; use localhost for Docker-exposed Redis.
# For production: set REDIS_URL to a full URL (e.g. Railway Redis or Upstash).
# REDIS_URL=redis://default:PASSWORD@host:6379
# Or use host/port (local Docker):
REDIS_HOST=localhost
REDIS_PORT=6379
# REDIS_PASSWORD=

# Server
PORT=3000

# CORS: frontend origin for credentialed requests (e.g. Bearer token). Default in code is http://localhost:8080 if unset.
CORS_ORIGIN=http://localhost:8080

# Embeddings (for document chunk indexing)
# EMBEDDING_DIMENSION=1536
# EMBEDDING_PROVIDER=stub
# For OpenAI: EMBEDDING_PROVIDER=openai, OPENAI_API_KEY=sk-...
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small
# Note: migration uses vector(1536); if you change EMBEDDING_DIMENSION, add a new migration to alter the column.

# RAG / LLM (for document chat)
# Context construction: per-chunk truncation and total context cap
# MAX_CHUNK_CHARS=2000
# MAX_CONTEXT_CHARS=8000
# LLM_PROVIDER=stub
# Stub: no API key; returns placeholder. Use for tests or when no LLM is configured.
# Gemini (default): streaming and non-streaming. Set GEMINI_API_KEY; GEMINI_MODEL optional.
# Ollama: LLM_PROVIDER=ollama, OLLAMA_BASE_URL, OLLAMA_MODEL for local streaming.
# OpenAI: non-streaming only; streaming throws 501. Use POST /documents/:id/chat for non-streaming.
LLM_PROVIDER=gemini
GEMINI_API_KEY=
GEMINI_MODEL=gemini-2.5-flash
# Ollama: LLM_PROVIDER=ollama
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=qwen2.5:7b
# OpenAI (non-streaming): OPENAI_API_KEY=sk-..., OPENAI_CHAT_MODEL=gpt-4o-mini
